{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM5IUVHCs3UAfmDUcl4vQqX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhuguvi26/Copy-of-A-Comprehensive-ETL-Workflow-with-Python-for-Data-Engineers/blob/main/Copy_of_A_Comprehensive_ETL_Workflow_with_Python_for_Data_Engineers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXV7sBV87LOD",
        "outputId": "22629e5c-07b4-487b-af0d-972e7d70375a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è Downloading files...\n",
            "\n",
            "‚úÖ Downloaded: source1.csv\n",
            "‚úÖ Downloaded: source2.csv\n",
            "‚úÖ Downloaded: source3.csv\n",
            "‚úÖ Downloaded: source1.json\n",
            "‚úÖ Downloaded: source2.json\n",
            "‚úÖ Downloaded: source3.json\n",
            "‚úÖ Downloaded: source1.xml\n",
            "‚úÖ Downloaded: source2.xml\n",
            "‚úÖ Downloaded: source3.xml\n",
            "\n",
            "üìÅ Download complete.\n",
            "\n",
            "üì§ All files extracted successfully.\n",
            "\n",
            "üîß Transforming...\n",
            "\n",
            "üíæ Loading into SQLite...\n",
            "\n",
            "üéâ Load complete ‚Äî etl_output.db created.\n",
            "\n",
            "üöÄ ETL Pipeline Completed Successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1234657921.py:113: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
            "  df[col] = pd.to_numeric(df[col], errors=\"ignore\")\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import requests\n",
        "import xml.etree.ElementTree as ET\n",
        "import sqlite3\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# CONFIG\n",
        "# -------------------------------------------------------\n",
        "BASE_URL = \"https://raw.githubusercontent.com/bhuguvi26/Copy-of-A-Comprehensive-ETL-Workflow-with-Python-for-Data-Engineers/main/source%20(3)/\"\n",
        "\n",
        "FILES = [\n",
        "    \"source1.csv\", \"source2.csv\", \"source3.csv\",\n",
        "    \"source1.json\", \"source2.json\", \"source3.json\",\n",
        "    \"source1.xml\", \"source2.xml\", \"source3.xml\"\n",
        "]\n",
        "\n",
        "DOWNLOAD_DIR = \"downloads\"\n",
        "os.makedirs(DOWNLOAD_DIR, exist_ok=True)\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# DOWNLOAD FILES\n",
        "# -------------------------------------------------------\n",
        "def download_files():\n",
        "    downloaded = []\n",
        "    print(\"‚¨áÔ∏è Downloading files...\\n\")\n",
        "\n",
        "    for file in FILES:\n",
        "        url = BASE_URL + file\n",
        "        save = os.path.join(DOWNLOAD_DIR, file)\n",
        "\n",
        "        try:\n",
        "            r = requests.get(url)\n",
        "            if r.status_code == 200:\n",
        "                with open(save, \"wb\") as f:\n",
        "                    f.write(r.content)\n",
        "                print(f\"‚úÖ Downloaded: {file}\")\n",
        "                downloaded.append(file)\n",
        "            else:\n",
        "                print(f\"‚ùå Failed ({r.status_code}): {file}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error downloading {file}: {e}\")\n",
        "\n",
        "    if not downloaded:\n",
        "        sys.exit(\"‚ùå No files downloaded ‚Äî check filenames!\")\n",
        "\n",
        "    print(\"\\nüìÅ Download complete.\\n\")\n",
        "    return downloaded\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# JSON HANDLER (FIX FOR YOUR FILES)\n",
        "# -------------------------------------------------------\n",
        "def extract_json(path):\n",
        "    rows = []\n",
        "    with open(path, \"r\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            try:\n",
        "                rows.append(json.loads(line))\n",
        "            except:\n",
        "                pass\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# XML HANDLER\n",
        "# -------------------------------------------------------\n",
        "def extract_xml(path):\n",
        "    tree = ET.parse(path)\n",
        "    root = tree.getroot()\n",
        "    rows = []\n",
        "\n",
        "    for rec in root.findall(\"./record\"):\n",
        "        row = {child.tag: child.text for child in rec}\n",
        "        rows.append(row)\n",
        "\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# MAIN EXTRACT FUNCTION\n",
        "# -------------------------------------------------------\n",
        "def extract_all(downloaded):\n",
        "    dfs = []\n",
        "\n",
        "    for file in downloaded:\n",
        "        path = os.path.join(DOWNLOAD_DIR, file)\n",
        "\n",
        "        if file.endswith(\".csv\"):\n",
        "            dfs.append(pd.read_csv(path))\n",
        "\n",
        "        elif file.endswith(\".json\"):\n",
        "            dfs.append(extract_json(path))\n",
        "\n",
        "        elif file.endswith(\".xml\"):\n",
        "            dfs.append(extract_xml(path))\n",
        "\n",
        "    if not dfs:\n",
        "        raise ValueError(\"‚ùå No data extracted\")\n",
        "\n",
        "    print(\"üì§ All files extracted successfully.\\n\")\n",
        "    return pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# TRANSFORM\n",
        "# -------------------------------------------------------\n",
        "def transform(df):\n",
        "    print(\"üîß Transforming...\\n\")\n",
        "    df.columns = df.columns.str.lower().str.strip()\n",
        "    for col in df.columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors=\"ignore\")\n",
        "    return df\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# LOAD ‚Üí SQLITE\n",
        "# -------------------------------------------------------\n",
        "def load(df):\n",
        "    print(\"üíæ Loading into SQLite...\\n\")\n",
        "    conn = sqlite3.connect(\"etl_output.db\")\n",
        "    df.to_sql(\"people\", conn, if_exists=\"replace\", index=False)\n",
        "    conn.close()\n",
        "    print(\"üéâ Load complete ‚Äî etl_output.db created.\\n\")\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# RUN ETL\n",
        "# -------------------------------------------------------\n",
        "downloaded = download_files()\n",
        "df = extract_all(downloaded)\n",
        "df = transform(df)\n",
        "load(df)\n",
        "\n",
        "print(\"üöÄ ETL Pipeline Completed Successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ReadMe\n",
        "üìä ETL Pipeline in Python ‚Äî CSV, JSON, XML | Google Colab\n",
        "\n",
        "This project demonstrates a complete Extract, Transform, Load (ETL) workflow using Python in Google Colab.\n",
        "The pipeline extracts data from CSV, JSON, and XML formats, transforms height and weight units, and loads the cleaned data into a CSV file for analytics or database storage.\n",
        "\n",
        "üöÄ Project Overview\n",
        "‚úÖ Objective\n",
        "\n",
        "Build a production-style ETL pipeline that:\n",
        "\n",
        "Extracts data from multiple formats (CSV, JSON, XML)\n",
        "\n",
        "Transforms:\n",
        "\n",
        "Height ‚Üí meters\n",
        "\n",
        "Weight ‚Üí kilograms\n",
        "\n",
        "Logs all ETL steps\n",
        "\n",
        "Saves final clean dataset into transformed_data.csv\n",
        "\n",
        "‚úÖ Skills Used\n",
        "\n",
        "Python\n",
        "\n",
        "Pandas\n",
        "\n",
        "File handling (CSV, JSON, XML)\n",
        "\n",
        "Data transformation\n",
        "\n",
        "Logging for ETL tracking\n",
        "\n",
        "üìÅ Input Data Formats\n",
        "CSV Example\n",
        "name,height,weight\n",
        "alex,65.78,112.99\n",
        "ajay,71.52,136.49\n",
        "alice,69.4,153.03\n",
        "\n",
        "JSON Example\n",
        "{\"name\":\"jack\",\"height\":68.70,\"weight\":123.30}\n",
        "{\"name\":\"tom\",\"height\":69.80,\"weight\":141.49}\n",
        "\n",
        "XML Example\n",
        "<data>\n",
        "   <person>\n",
        "      <name>simon</name>\n",
        "      <height>67.90</height>\n",
        "      <weight>112.37</weight>\n",
        "   </person>\n",
        "</data>\n",
        "\n",
        "üì¶ Output\n",
        "transformed_data.csv preview:\n",
        "name\theight\tweight\theight_m\tweight_kg\n",
        "alex\t65.78\t112.99\t1.671\t51.251\n",
        "ajay\t71.52\t136.49\t1.817\t61.911\n",
        "alice\t69.40\t153.03\t1.763\t69.413\n",
        "Generated Log File\n",
        "\n",
        "etl_log.txt ‚Äî contains timestamped logs for each ETL phase.\n",
        "\n",
        "‚öôÔ∏è ETL Workflow\n",
        "1Ô∏è‚É£ Extract\n",
        "\n",
        "Reads all uploaded .csv, .json, .xml files and combines into a DataFrame.\n",
        "\n",
        "2Ô∏è‚É£ Transform\n",
        "\n",
        "Height (inches ‚Üí meters):\n",
        "height_m = height * 0.0254\n",
        "\n",
        "Weight (lbs ‚Üí kg):\n",
        "weight_kg = weight * 0.45359237\n",
        "\n",
        "3Ô∏è‚É£ Load\n",
        "\n",
        "Saves result to:\n",
        "\n",
        "transformed_data.csv\n",
        "\n",
        "üìé Running the Project in Google Colab\n",
        "Step 1 ‚Äî Upload Files\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "Step 2 ‚Äî Run Complete ETL Script\n",
        "\n",
        "Run the one-cell ETL code provided in this project.\n",
        "\n",
        "üß† Key Learnings\n",
        "\n",
        "ETL automation in Python\n",
        "\n",
        "Parsing structured data files\n",
        "\n",
        "Real-world logging practices\n",
        "\n",
        "Data cleaning & unit conversion\n",
        "\n",
        "‚úÖ Project Status\n",
        "\n",
        "‚úî Completed\n",
        "‚úî Tested with real data\n",
        "‚úî Production-style logging & modularity"
      ],
      "metadata": {
        "id": "7_hOL4XoEIGn"
      }
    }
  ]
}